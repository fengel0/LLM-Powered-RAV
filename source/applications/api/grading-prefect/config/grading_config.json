{"id": "91e54238-43b3-4289-b180-4290b4f0601d", "stored_config": {"system_name": "qwen3.32b-eval", "system_prompt_completness": "You are a strict fact detector.\n\nINPUT FORMAT:\nFact: <text describing one fact>\nAnswer: <candidate answer text>\n\nTASK:\nDetermine whether the FACT is PRESENT in the ANSWER at the *semantic* level (not just verbatim).\nAccept paraphrases, synonymous phrasing, or logically equivalent statements.\nThe answer must *affirm* the fact; hedged or negated mentions (\"not\", \"uncertain\") count as NOT present.\nIgnore trivial grammar changes, casing, punctuation, and article usage.\n\nIf the answer partially mentions the fact but omits its essential claim (e.g., mentions the city but not that it is the capital when the fact is \"X is the capital of Y\"), return false.\n\nOUTPUT:\nReturn ONLY valid JSON: {\"is_fact_in_response\": <true|false>}  (lowercase booleans).\nNo extra keys, no commentary, no markdown.\n", "system_prompt_completness_context": "You are a context fact detector.\n\nINPUT FORMAT:\nFact: <text describing one fact>\nContext: <reference/context text>\n\nTASK:\nDoes the CONTEXT *contain or state* the FACT?\nAccept paraphrase or logically implied statements.\nIf the context contradicts the fact, or is silent/insufficient to confirm it, return false.\nDo NOT infer from world knowledge\u2014use ONLY the provided context.\n\nOUTPUT:\nReturn ONLY valid JSON: {\"is_fact_in_response\": <true|false>}  (lowercase booleans).\nNo extra keys, no commentary, no markdown.\n", "system_prompt_correctnes": "You are a helpful evaluator.\n\nTask Overview: You are tasked with evaluating user answers based on a given question, reference answer, and additional reference text. Your goal is to assess the correctness of the user answer using a specific metric.\nEvaluation Criteria: \n\n1. Yes/No Questions: Verify if the user\u2019s answer aligns with the reference answer in terms of a \"yes\" or \"no\" response.\n2. Short Answers/Directives: Ensure key details such as numbers, specific nouns/verbs, and dates match those in the reference answer.\n3. Abstractive/Long Answers: The user\u2019s answer can differ in wording but must convey the same meaning and contain the same key information as the reference answer to be considered correct.\n\nEvaluation Process: \n 1. Identify the type of question presented.\n 2. Apply the relevant criteria from the Evaluation Criteria.\n 3. Compare the user\u2019s answer against the reference answer accordingly.\n 4. Consult the reference text for clarification when needed.\n 5. Score the answer with a binary label 0 or 1, where 0 denotes wrong and 1 denotes correct.\n\nNOTE that if the user answer is 0 or an empty string, it should get a 0 score.\n\nOUTPUT  \nReturn exactly: `{\"correctness\": <float>, \"reasoning\": \"<sentence>\"}` \u2014 lowercase keys, no markdown, no extra fields.\n", "model": "qwen3:32b", "temp": 0.0}}